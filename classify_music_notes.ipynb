{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file and dataframe to store accuracy and loss of modelfit epochs\n",
    "# The code in this cell was commented out after creating the dataframe for all 8 optimizers\n",
    "\n",
    "# file = \"modelfit_history.csv\"\n",
    "# modelfit_hist_df = pd.read_csv(file)\n",
    "# modelfit_hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process image data using CV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data is found in the kaggle zip file https://www.kaggle.com/kishanj/music-notes-datasets\n",
    "- Zip file was extracted.\n",
    "- All of the image files were in folders for by note type (Whole, Half, Quarter, Eight, Sixteenth)\n",
    "- In order to simplify the code, all files were copied out of their subdirectories into single \"Images\" folder. \n",
    "- The Images folder is part of gitignore, and therefore unavailable from our git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the Images folder and process each image file\n",
    "# Append the image to a list and append the class name to a separate list\n",
    "\n",
    "img_data_array=[]\n",
    "class_name=[]\n",
    "folder_name = \"Images\"\n",
    "\n",
    "# This code based on https://towardsdatascience.com/loading-custom-image-dataset-for-deep-learning-models-part-1-d64fa7aaeca6\n",
    "# This is the code that required us to install OpenCV2\n",
    "# This code also handles the scaling needed\n",
    "\n",
    "for file in os.listdir(os.path.join(folder_name)):\n",
    "    \n",
    "    image_path= os.path.join(folder_name,  file)\n",
    "    image= cv2.imread( image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    #image=np.array(image)  ????\n",
    "\n",
    "    image = image.astype('float32')\n",
    "    image /= 255 #this gets black and white to 1s and 0s \n",
    "    img_data_array.append(image)\n",
    "    note_class = file[0:1]\n",
    "\n",
    "    class_name.append(note_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(notes_folder):\n",
    "    class_name = []\n",
    "    img_data_array = []\n",
    "    \n",
    "    for note_dir in os.listdir(notes_folder):\n",
    "        \n",
    "        for file in os.listdir(os.path.join(notes_folder, note_dir)):\n",
    "            image = os.path.join(notes_folder, note_dir, file)\n",
    "            image = cv2.imread(notes_folder, cv2.COLOR_BGR2RGB)\n",
    "            #image = cv2.resize( image, (IMG_HEIGHT, IMG_WIDTH), interpolation = cv2.INTER_AREA)\n",
    "            #image = np.array(image)  # converts the image to a numpy array\n",
    "            image = image.astype('float32')\n",
    "            image /= 255\n",
    "            img_data_array.append(image)\n",
    "            class_name.append(note_dir)\n",
    "            \n",
    "    print(img_data_array[0])\n",
    "    return img_data_array, class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a unique number to each class\n",
    "\n",
    "note_classifier_dict = {key:value for value, key in enumerate(np.unique(class_name))}\n",
    "\n",
    "note_classifier_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is y, converting the class names to a numeric value for all values\n",
    "\n",
    "target_val = [note_classifier_dict[class_name[i]] for i in range(len(class_name))]\n",
    "\n",
    "# target_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the image array ... setting the threshold to 4096 or above avoids truncating the array\n",
    "\n",
    "np.set_printoptions(threshold=4096)\n",
    "\n",
    "img_data_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to make sure our images are still good\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img_data_array[10], cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 64x64 pixels to 1D array \n",
    "# This is similar to what was done in MNIST class example\n",
    "\n",
    "num_dimensions = 64*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this an np array so we can use reshape on our array to flatten\n",
    "# the array from 64 x 64 to 1 x 4096 (from a vector matrix to a scalar matrix)\n",
    "\n",
    "x=np.array(img_data_array, np.float32)\n",
    "\n",
    "reshape_test = x.reshape(x.shape[0],num_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reshape_test,target_val, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *NOTE*:  Instead of using the code in this cell, we performed the scaling manually above \n",
    "# using 'image /= 255' as we loaded each image \n",
    "\n",
    "# Next, we normalize our training data to be between 0 and 1\n",
    "# scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Training and Testing labels are integer encoded from 0 to 4\n",
    "y_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have five categories: whole, half, quarter, eighth, and sixteenth notes\n",
    "num_classes = 5\n",
    "\n",
    "# Encode the target using one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first layer where the input dimensions are 4096 pixels\n",
    "# Activation function using 'relu' for the hidden layers and 'softmax' for the output layer\n",
    "# Each of the hidden layers are densely connected and have 100 nodes per layer\n",
    "model.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a second hidden layer with 100 densely connected nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(100, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final output layer uses softmax activation function for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train our Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were run one at a time, with only one optimizer in play for each run\n",
    "All of the optimizers were run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = \"Adam\"\n",
    "# optimizer = \"SGD\"\n",
    "# optimizer = \"RMSprop\"\n",
    "# optimizer = \"Adadelta\"\n",
    "# optimizer = \"Adagrad\"\n",
    "# optimizer = \"Adamax\"\n",
    "# optimizer = \"Nadam\"\n",
    "optimizer = tf.keras.optimizers.Ftrl(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (fit) our model using the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are setting the model.fit() equal to a variable 'history' in order to use it to create the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit_hist_df=pd.DataFrame()   #only needed for first run\n",
    "modelfit_hist_df[\"ftrl_loss\"] = history.history['loss']  #only needed for last one\n",
    "modelfit_hist_df[\"ftrl_acc\"] = history.history['acc']   #only needed for last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelfit_hist_df[optimizer+\"_loss\"] = history.history['loss']\n",
    "# modelfit_hist_df[optimizer+\"_acc\"] = history.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelfit_hist_df #test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelfit_hist_df.to_csv(file, index=False)\n",
    "modelfit_hist_df.to_csv('FTRL_ACC_LOSS.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history.model #testing output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the Optimizer performance\n",
    "The loss and accuracy for each optimizer and save each plot as a file and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Loss (training data)')\n",
    "# plt.plot(history.history['acc'], label='MAE (validation data)')\n",
    "plt.plot(history.history['acc'], label='Accuracy (training data)')\n",
    "#plt.title('Loss and Accuracy vs. Epochs - Optimizer: '+ optimizer)\n",
    "plt.title('Loss and Accuracy vs. Epochs - Optimizer: FTRL')\n",
    "plt.ylabel('Model Efficiency')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc=\"middle\")\n",
    "#plt.savefig(\"Results/100epoch_relu_\" + optimizer + \".jpeg\")\n",
    "plt.savefig(\"Results/100epoch_relu_FTRL.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out so we don't resave over existing file\n",
    "#model.save(\"Results/Notes_relu_\" + optimizer + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Evaluate each of the 8 optimizers and store the loss and accuracy in a new DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define list to hold model eval results\n",
    "df_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test,y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predictions(results_folder):\n",
    "\n",
    "    \n",
    "     for model_file in os.listdir(os.path.join(results_folder)):\n",
    "            if \"h5\" in model_file:\n",
    "                print(\"working on file: \"+model_file)\n",
    "                model = load_model(os.path.join(results_folder,model_file))\n",
    "#                 break #for testing purposes only\n",
    "                try:\n",
    "                    model_loss, model_accuracy = model.evaluate(X_test,y_test, verbose=2)\n",
    "                    print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "                    df_list.append([model_file, model_loss,model_accuracy])\n",
    "                except:\n",
    "                    print(\"Ftrl model defies being saved and will not load. filename:\"+model_file)\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions('Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put results to df\n",
    "print(df_list)\n",
    "results_df = pd.DataFrame(df_list,columns=[\"Model\",\"Loss\",\"Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv for later loading/graphing elsewhere\n",
    "results_df.to_csv(os.path.join('Results','Model_predict_loss_acc.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
